{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Получение эмбеддингов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импортирование необходимых бибилиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "import tensorflow_hub as hub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the ultimate guide to ielts writing about the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by pioneer bdp180 network 3d bluray player wit...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vkcreation womens new designer silk semistiche...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>portronics sound slick ii por936 wireless blue...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rebuilding india were the last four years tran...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Label\n",
       "0  the ultimate guide to ielts writing about the ...      1\n",
       "1  by pioneer bdp180 network 3d bluray player wit...      3\n",
       "2  vkcreation womens new designer silk semistiche...      2\n",
       "3  portronics sound slick ii por936 wireless blue...      3\n",
       "4  rebuilding india were the last four years tran...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.rename(columns={'Description': 'text'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Первичный анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анализ данных:\n",
      "--------------------------\n",
      "Исходный размер выборки: 26330\n",
      "Количество пустых строк (пропусков): 0\n",
      "Количество дубликатов в датафрейме: 0\n",
      "--------------------------\n",
      "Итоговый размер выборки: 26330\n"
     ]
    }
   ],
   "source": [
    "# Подсчет пропусков (пустых строк) и дубликатов\n",
    "count_missing = df['text'].isna().sum() + (df['text'] == '').sum()\n",
    "count_duplicates = df.duplicated().sum()\n",
    "\n",
    "print(\"Анализ данных:\")\n",
    "print(\"--------------------------\")\n",
    "print(f\"Исходный размер выборки: {len(df)}\")\n",
    "print(f\"Количество пустых строк (пропусков): {count_missing}\")\n",
    "print(f\"Количество дубликатов в датафрейме: {count_duplicates}\")\n",
    "\n",
    "# Удаление дубликатов и обновление размера выборки\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "print(\"--------------------------\")\n",
    "print(f\"Итоговый размер выборки: {len(df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>prep_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the ultimate guide to ielts writing about the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ultimate guide ielts write book ultimate guide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by pioneer bdp180 network 3d bluray player wit...</td>\n",
       "      <td>3</td>\n",
       "      <td>pioneer bdp180 network 3d bluray player 4k vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vkcreation womens new designer silk semistiche...</td>\n",
       "      <td>2</td>\n",
       "      <td>vkcreation womens new designer silk semistiche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>portronics sound slick ii por936 wireless blue...</td>\n",
       "      <td>3</td>\n",
       "      <td>portronics sound slick ii por936 wireless blue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rebuilding india were the last four years tran...</td>\n",
       "      <td>1</td>\n",
       "      <td>rebuild india last four year transformative en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Label  \\\n",
       "0  the ultimate guide to ielts writing about the ...      1   \n",
       "1  by pioneer bdp180 network 3d bluray player wit...      3   \n",
       "2  vkcreation womens new designer silk semistiche...      2   \n",
       "3  portronics sound slick ii por936 wireless blue...      3   \n",
       "4  rebuilding india were the last four years tran...      1   \n",
       "\n",
       "                                           prep_text  \n",
       "0  ultimate guide ielts write book ultimate guide...  \n",
       "1  pioneer bdp180 network 3d bluray player 4k vid...  \n",
       "2  vkcreation womens new designer silk semistiche...  \n",
       "3  portronics sound slick ii por936 wireless blue...  \n",
       "4  rebuild india last four year transformative en...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Приведение текста к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление лишних пробелов\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Токенизация текста\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    \n",
    "    # Удаление знаков препинания\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Лемматизация токенов\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        pos = tag[0].lower() if tag[0].lower() in ['a', 'r', 'n', 'v'] else 'n'\n",
    "        lemmatized_tokens.append(wordnet.lemmatize(token, pos))\n",
    "    \n",
    "    # Объединение лемматизированных токенов в строку\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "df['prep_text'] = df['text'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка LLM моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Matvey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Matvey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Matvey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Matvey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "model_1_all_mpnet_base_v2 = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "model_2_all_MiniLM_L6_v2 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model_3_USE = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание датафреймов с энбеддингами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26330/26330 [43:41<00:00, 10.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26330/26330 [09:31<00:00, 46.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26330/26330 [00:51<00:00, 507.32it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_embedding(model_type, text, model_idx):\n",
    "    if model_idx == 2:\n",
    "        embeddings = model_type([text])\n",
    "        return embeddings.numpy()[0] \n",
    "    else:\n",
    "        embeddings = model_type.encode([text])  \n",
    "        return embeddings[0]\n",
    "\n",
    "models_list = [\n",
    "    model_1_all_mpnet_base_v2,\n",
    "    model_2_all_MiniLM_L6_v2,\n",
    "    model_3_USE\n",
    "]\n",
    "\n",
    "tqdm.pandas()  \n",
    "\n",
    "# Создание датафрейма с эмбеддингами исходного текста\n",
    "for model_idx, model_type in enumerate(models_list):\n",
    "    new_col_name = f\"{model_idx + 1}_text\"\n",
    "    print(new_col_name)\n",
    "    df[new_col_name] = df['text'].progress_apply(lambda text: get_sentence_embedding(model_type, text, model_idx))\n",
    "\n",
    "df.to_csv('all_embeddings_text.csv')\n",
    "\n",
    "# Создание датафрейма с эмбеддингами предобработанного текста\n",
    "for model_idx, model_type in enumerate(models_list):\n",
    "    new_col_name = f\"{model_idx + 1}_prep_text\"\n",
    "    print(new_col_name)\n",
    "    df[new_col_name] = df['prep_text'].progress_apply(lambda text: get_sentence_embedding(model_type, text, model_idx))\n",
    "\n",
    "df.to_csv('all_embeddings_prep_text.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
